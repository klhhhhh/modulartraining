{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertConfig\n",
    "from transformers import BertTokenizer\n",
    "import datasets\n",
    "import json\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "config = BertConfig()\n",
    "model = BertForMaskedLM(config)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',use_fast=True) \n",
    "model = BertForMaskedLM.from_pretrained('/home/wanzhipeng/deepincubation/bert_wiki/checkpoint-145000')\n",
    "# model_checkpoint = \"distilroberta-base\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "# tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "# model = BertForMaskedLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0594, -0.1679, -0.1557, -1.9913,  1.2222, -0.1535],\n",
       "        [ 0.8664,  0.3704,  0.2489,  0.4679, -0.4451,  0.8060],\n",
       "        [ 0.0000, -1.4458,  2.2982, -1.4787,  1.1293, -1.5805],\n",
       "        [ 0.0000,  0.0000,  1.1288, -0.2951,  1.2854,  0.0497]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn([3,3,4])\n",
    "a\n",
    "torch.triu(a)\n",
    "print(torch.triu(a))\n",
    "torch.triu(a, diagonal=1)\n",
    "torch.triu(a, diagonal=-1)\n",
    "\n",
    "b = torch.randn(4, 6)\n",
    "b\n",
    "torch.triu(b, diagonal=1)\n",
    "torch.triu(b, diagonal=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/wanzhipeng/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3dbc3a6a8b436aa8c491bac9d7eb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wanzhipeng/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a0aeeb4e4600b55f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/wanzhipeng/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-8491a52215aa65e0_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/wanzhipeng/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-fe40a1f675cf94d1_*_of_00004.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1027, 11748, 4801, 4360, 11906, 3523, 1027, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"cc_news\", split=\"train\")\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "tokenized_datasets[\"train\"][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wanzhipeng/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-fc031ffbf5d3610d_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/wanzhipeng/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-4ba82bf0833b067d_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/wanzhipeng/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6b0c8a5650ec01b9_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2199\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 18549\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1921\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the first game and follows the \" nameless \", a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \". [SEP] [CLS] the game began development in 2010, carrying over a large portion of the work done on valkyria chronicles ii. while it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. character designer raita honjou and composer hitoshi sakimoto both returned from previous entries, along with valkyria chronicles ii director takeshi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_wiki\",          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    logging_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=500,            # number of training epochs, feel free to tweak\n",
    "    # per_device_train_batch_size=1000, # the training batch size, put it as high as your GPU memory fits\n",
    "    # gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    # per_device_eval_batch_size=1000,  # evaluation batch size\n",
    "    logging_steps=500,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=500,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10000\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"fuckthelife\"\n",
    "# training_args = TrainingArguments(\n",
    "#     f\"{model_name}-finetuned-wikitext2\",\n",
    "#     evaluation_strategy = \"steps\",\n",
    "#     logging_strategy=\"steps\",\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(\"cuda\"),\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 18549\n",
      "})\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets[\"train\"])\n",
    "print(len(lm_datasets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanzhipeng/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71891a5770b40d9aabeded6b5476f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanzhipeng/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='98306' max='145000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 98306/145000 02:32 < 6:29:19, 2.00 it/s, Epoch 338.98/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.47138166427612305,\n",
       "  'token': 2944,\n",
       "  'token_str': 'model',\n",
       "  'sequence': \"hello i'm a model model.\"},\n",
       " {'score': 0.17571479082107544,\n",
       "  'token': 7966,\n",
       "  'token_str': 'fool',\n",
       "  'sequence': \"hello i'm a fool model.\"},\n",
       " {'score': 0.052288252860307693,\n",
       "  'token': 2217,\n",
       "  'token_str': 'side',\n",
       "  'sequence': \"hello i'm a side model.\"},\n",
       " {'score': 0.039750780910253525,\n",
       "  'token': 2047,\n",
       "  'token_str': 'new',\n",
       "  'sequence': \"hello i'm a new model.\"},\n",
       " {'score': 0.016525108367204666,\n",
       "  'token': 4190,\n",
       "  'token_str': 'leg',\n",
       "  'sequence': \"hello i'm a leg model.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model=model.to(\"cpu\"),tokenizer=tokenizer)\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 114.16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "model = model.to(\"cuda\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
